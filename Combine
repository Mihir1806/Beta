import pandas as pd
import os
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

class DataAnalysisBot:
    def __init__(self):
        self.master_data = None
        self.column_descriptions = {}
        self.model = None
    
    # Phase 1: Combine multiple Excel files into a master dataset
    def combine_excel_files(self, folder_path):
        all_files = [file for file in os.listdir(folder_path) if file.endswith('.xlsx')]
        dataframes = [pd.read_excel(os.path.join(folder_path, file)) for file in all_files]
        self.master_data = pd.concat(dataframes, ignore_index=True)
        print("Master data created with shape:", self.master_data.shape)
    
    # Phase 2: Define the structure of the master data in detail
    def define_column_structure(self, column_info):
        """
        This function allows the user to provide detailed descriptions for each column in the master dataset.
        It also validates whether the expected columns exist in the master dataset and reports missing ones.
        
        Parameters:
        column_info (dict): A dictionary where keys are column names and values are descriptions.
        """
        self.column_descriptions = column_info
        
        missing_columns = [col for col in column_info.keys() if col not in self.master_data.columns]
        
        if missing_columns:
            print("Warning: The following columns are missing from the master data:", missing_columns)
        else:
            print("All expected columns are present in the master data.")
        
        print("Column structure defined with descriptions:")
        for col, desc in column_info.items():
            print(f"{col}: {desc}")
    
    # Phase 3: Compare transaction data with master data and detect issues
    def analyze_transactions(self, transaction_file):
        transaction_data = pd.read_excel(transaction_file)
        issues = []
        
        for col in self.master_data.columns:
            if col not in transaction_data.columns:
                issues.append(f"Missing column: {col}")
            else:
                invalid_entries = transaction_data[~transaction_data[col].isin(self.master_data[col])]
                if not invalid_entries.empty:
                    issues.append(f"Invalid values in column {col}: {invalid_entries[col].unique()}")
        
        return issues
    
    # Phase 4: Train a model on master data and retrain with new cases
    def train_model(self, target_column):
        if target_column not in self.master_data.columns:
            print("Error: Target column not found in master data")
            return
        
        X = self.master_data.drop(columns=[target_column])
        y = self.master_data[target_column]
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        
        self.model = DecisionTreeClassifier()
        self.model.fit(X_train, y_train)
        
        y_pred = self.model.predict(X_test)
        print("Model accuracy:", accuracy_score(y_test, y_pred))
    
    def retrain_model(self, new_data):
        if self.model is None:
            print("No existing model found. Please train the model first.")
            return
        
        X_new = new_data.drop(columns=[target_column])
        y_new = new_data[target_column]
        
        self.model.fit(X_new, y_new)
        print("Model retrained with new data.")

# Example usage
bot = DataAnalysisBot()
bot.combine_excel_files("./data")
bot.define_column_structure({"column1": "Description", "column2": "Another Description"})
issues = bot.analyze_transactions("transaction.xlsx")
print("Issues found:", issues)
bot.train_model("target_column")
